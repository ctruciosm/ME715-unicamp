---
title: "ME715 - Econometria"
subtitle: "Modelo de Regressão Linear IV"
author: "Prof. Carlos Trucíos </br> ctrucios@unicamp.br"
Email: "ctrucios@unicamp.br"
institute: "Instituto de Matemática, Estatística e Computação Científica (IMECC), </br> Universidade Estadual de Campinas (UNICAMP)."
knitr:
    opts_chunk: 
      fig.align: 'center'
execute:
    message: false
    warning: false
format:
    revealjs:
        slide-number: true
        show-slide-number: print
        self-contained: false
        chalkboard: true
        width: 1600
        height: 900
        theme: [default, styles.scss]
        incremental: true
        code-fold: true
        logo: "imagens/imecc.png"
        footer: "Carlos Trucíos (IMECC/UNICAMP)  |  ME715 - Econometria  |  [ctruciosm.github.io](https://ctruciosm.github.io/)"
        highlight-style: "a11y"
        title-slide-attributes:
            data-background-image: "imagens/unicamp.png"
            data-background-size: 20%
            data-background-position: 99% 5%
            data-background-opacity: "1"
---

# Variáveis irrelevantes no modelo de regressão
## Variáveis irrelevantes no modelo de regressão


Suponha que o modelo populacional seja $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + u.$$

. . . 

Contudo, na hora de estimar, trabalhamos como se o modelo populacional fosse $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + u.$$


. . . 

<center>
![](imagens/duvida.png)

**Quais os efeitos de se fazer isto?**
</center>


## Variáveis irrelevantes no modelo de regressão

$$\text{Sabemos que} \quad \mathbb{E}(\hat{\beta}) = \beta$$

Ou seja: 

- $\mathbb{E}(\hat{\beta}_0) = \beta_0$, 
- $\mathbb{E}(\hat{\beta}_1) = \beta_1$,
- $\mathbb{E}(\hat{\beta}_2) = \beta_2$,
- $\mathbb{E}(\hat{\beta}_3) = \beta_3 = 0$


. . . 

<center>
> [**Incluir uma ou mais variáveis irrelevantes no modelo não afeta a inexistência de viés dos EMQO**]{style="color:red;"}

</center>


. . . 

Isso significa que podemos incluir tantas variáveis quanto quisermos sem nenhuma _"punição"_? 

## Variáveis irrelevantes no modelo de regressão

<center>
> [Incluir uma ou mais variáveis irrelevantes no modelo não afeta a inexistência de viés dos EMQO.]{style="color:red;"} [**Contudo, incluir variáveis irrelevantes tem efeitos indesejáveis sobre as variâncias dos EMQO (ver multicolinearidade).**]{style="color:blue;"}

</center>




## Variáveis irrelevantes no modelo de regressão


```{r}
#| echo: true
library(MASS)
n <- 1000
betas <- matrix(NA, ncol = 3, nrow = 1000)
betas_i <- matrix(NA, ncol = 4, nrow = 1000)
for (i in 1:1000) {
  u <- rnorm(n)
  x1 <- runif(n)
  x <- mvrnorm(n, mu = c(0,0), Sigma = matrix(c(1, 0.5, 0.5, 2), 2, 2))
  x2 <- x[,1]
  x3 <- x[,2]
  y <- 0.7 + 0.6*x1 - 0.2*x2 + u
  betas[i, ] <- coef(lm(y ~ x1 + x2))
  betas_i[i, ] <- coef(lm(y ~ x1 + x2 + x3))
}
par(mfrow = c(1, 2))
boxplot(betas)
title("Boxplot para betas")
boxplot(betas_i)
title("Boxplot para betas + irrelevantes")
```


## Variáveis irrelevantes no modelo de regressão


::: {.callout-tip}

## Teorema FWL (Frisch-Waugh-Lovell)

Sejam os modelos $$Y = \textbf{X}_1\beta_1 + \textbf{X}_2 \beta_2 + u \quad e \quad M_1Y = M_1 \textbf{X}_2 \beta_2 + \nu,$$ em que $M_1 = \textbf{I}- X_1(X_1'X_1)^{-1}X_1'$


1. $\hat{\beta_2}$ em ambas as regressões é numericamente idêntico.
2. $\hat{u}$ e $\hat{\nu}$ são numericamente idênticos.

:::

**Demostração** (ver lista)


## Variáveis irrelevantes no modelo de regressão


Suponha que estimamos o modelo $$Y = \textbf{X}\delta + \textbf{Z}\gamma + u, \quad u \sim IID(0, \sigma^2I )$$

mas o verdadeiro modelo é $$Y = \textbf{X}\beta + u,  \quad u \sim IID(0, \sigma^2I ).$$


. . . 


- Pelo teorema FWL, $\hat{\delta} = (X'M_z X)^{-1} X' M_zY$.
- $\hat{\delta} = (X'M_z X)^{-1} X' M_z(X \beta + u)$
- $\hat{\delta} = \beta + (X'M_z X)^{-1} X' M_z u$
- $\mathbb{E}(\hat{\delta}|X,Z) = \beta + (X'M_z X)^{-1} X' M_z \underbrace{\mathbb{E}(u|X,Z)}_{0}$
- $\mathbb{E}(\hat{\delta}) = \beta$

## Variáveis irrelevantes no modelo de regressão

<center>
Então podemos incluir variáveis sem nenhum prejuizo?
</center>

Não!.

- $\mathbb{V}(\hat{\delta}|X,Z) = \sigma^2 (X' M_z X)^{-1}$.
- $\mathbb{V}(\hat{\delta}|X,Z) \geq \mathbb{V}(\hat{\beta}|X)$.

. . . 

- Provar que $\mathbb{V}(\hat{\delta}|X,Z) \geq \mathbb{V}(\hat{\beta}|X)$ é equivalente a provar que $\mathbb{V}(\hat{\beta}|X)^{-1} \geq \mathbb{V}(\hat{\delta}|X,Z)^{-1}$
- Mas $X'X - X'M_zX = X'(I - M_z)X = X'P_z'P_zX = (P_z X)'P_zX \geq 0$.
- Então, $\underbrace{\sigma^{-2} X'X}_{\mathbb{V}(\hat{\beta}|X)^{-1}} - \underbrace{\sigma^{-2}X'M_zX}_{\mathbb{V}(\hat{\delta}|X,Z)^{-1}} \geq 0$.
- Logo, $\mathbb{V}(\hat{\delta}|X,Z) \geq \mathbb{V}(\hat{\beta}|X)$.


# Variáveis omitidas
## Variáveis omitidas



Suponha que o modelo populacional seja $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + u.$$

. . . 

Contudo, na hora de estimar, trabalhamos como se o modelo populacional fosse $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + u.$$


. . . 

<center>
![](imagens/duvida.png)

**Quais os efeitos de se fazer isto?**
</center>


## Variáveis omitidas


```{r}
#| echo: true
library(MASS)
n <- 1000
betas <- matrix(NA, ncol = 4, nrow = 1000)
betas_o <- matrix(NA, ncol = 3, nrow = 1000)
for (i in 1:1000) {
  u <- rnorm(n)
  x1 <- runif(n)
  x <- mvrnorm(n, mu = c(0,0), Sigma = matrix(c(1, 0.5, 0.5, 2), 2, 2))
  x2 <- x[,1]
  x3 <- x[,2]
  y <- 0.7 + 0.6*x1 - 0.2*x2 + 0.9*x3 + u
  betas[i, ] <- coef(lm(y ~ x1 + x2 + x3))
  betas_o[i, ] <- coef(lm(y ~ x1 + x2))
}
par(mfrow = c(1, 2))
boxplot(betas, ylim = c(-0.3, 1))
title("Boxplot para betas")
boxplot(betas_o, ylim = c(-0.3, 1))
title("Boxplot para betas - omitidas")
```



## Variáveis omitidas


```{r}
#| echo: true
library(MASS)
n <- 1000
betas <- matrix(NA, ncol = 4, nrow = 1000)
betas_o <- matrix(NA, ncol = 3, nrow = 1000)
for (i in 1:1000) {
  u <- rnorm(n)
  x <- mvrnorm(n, mu = c(0,0,0), Sigma = matrix(c(1, 0.5, -0.7, 0.5, 2, 0.3, -0.7, 0.3, 1), 3, 3))
  x1 <- x[,1]
  x2 <- x[,2]
  x3 <- x[,3]
  y <- 0.7 + 0.6*x1 - 0.2*x2 + 0.9*x3 + u
  betas[i, ] <- coef(lm(y ~ x1 + x2 + x3))
  betas_o[i, ] <- coef(lm(y ~ x1 + x2))
}
par(mfrow = c(1, 2))
boxplot(betas, ylim = c(-0.3, 1))
title("Boxplot para betas")
boxplot(betas_o, ylim = c(-0.3, 1))
title("Boxplot para betas - omitidas")
```



## Variáveis omitidas


```{r}
#| echo: true
library(MASS)
n <- 1000
betas <- matrix(NA, ncol = 4, nrow = 1000)
betas_o <- matrix(NA, ncol = 3, nrow = 1000)
for (i in 1:1000) {
  u <- rnorm(n)
  x <- mvrnorm(n, mu = c(0,0,0), Sigma = matrix(c(1, 0.5, 0, 0.5, 1, 0.3, 0, 0.3, 1), 3, 3))
  x1 <- x[,1]
  x2 <- x[,2]
  x3 <- x[,3]
  y <- 0.7 + 0.6*x1 - 0.2*x2 + 0.9*x3 + u
  betas[i, ] <- coef(lm(y ~ x1 + x2 + x3))
  betas_o[i, ] <- coef(lm(y ~ x1 + x2))
}
par(mfrow = c(1, 2))
boxplot(betas, ylim = c(-0.3, 1))
title("Boxplot para betas")
boxplot(betas_o, ylim = c(-0.3, 1))
title("Boxplot para betas - omitidas")
```



## Variáveis omitidas

<center>
[Variáveis omitidas levarão a estimadores $\tilde{\beta}$ viesados.]{style="color:red;"}
</center>


Suponha que o modelo populacional seja $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + u.$$

. . . 

Contudo, ajustamos o modelo considerando $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + u.$$

. . . 

- Suponha que $X_2$ e $X_3$ sejam não correlacionados mas que $X_1$ e $X_3$ sejam correlacionados.
- É tentador pensar que como $X_3$ não está correlacionada com $X_2$, $\tilde{\beta}_2$ será não viesado, mas não é isso que acontece.
- O único caso em que isso acontece é se $X_1$ e $X_2$ também forem não correlacionados.

. . . 

**Muitas vezes é dificil determinar o sinal do vies, mas o importante aqui é saber que, resultados contraintuitivos, podem ser fruto de estimadores viesados.**


## Variáveis omitidas

Sejam $\textbf{X} = [\textbf{X}_1 \textbf{X}_2]$ e $\beta = [\beta_1 \beta_2]'$ e seja o modelo populacional

$$Y = \textbf{X} \beta + u$$

. . . 

- Vamos supor que trabalhamos como se o modelo fosse $Y = X_1 \beta_1 + u$.
- Em geral, $\mathbb{E}(\hat{\beta}_1) \neq \beta_1$
- $\mathbb{E}(\hat{\beta}_1) = \beta_1$ se $\beta_2 = 0$ ou $X_1'X_2 = 0$ (ortogonal). Se $X_1$ e $X_2$ forem centradas, é equivalente a dizer que a correlação deve ser 0.


## Variáveis omitidas


```{r}
#| echo: true
library(MASS)
n <- 1000
betas <- matrix(NA, ncol = 4, nrow = 1000)
betas_o <- matrix(NA, ncol = 3, nrow = 1000)
for (i in 1:1000) {
  u <- rnorm(n)
  x <- mvrnorm(n, mu = c(0,0,0), Sigma = matrix(c(1, 0, 0, 0, 1, 0, 0, 0, 1), 3, 3))
  x1 <- x[,1]
  x2 <- x[,2]
  x3 <- x[,3]
  y <- 0.7 + 0.6*x1 - 0.2*x2 + 0.9*x3 + u
  betas[i, ] <- coef(lm(y ~ x1 + x2 + x3))
  betas_o[i, ] <- coef(lm(y ~ x1 + x2))
}
par(mfrow = c(1, 2))
boxplot(betas, ylim = c(-0.3, 1))
title("Boxplot para betas")
boxplot(betas_o, ylim = c(-0.3, 1))
title("Boxplot para betas - omitidas")
```


# Multicolinearidade
## Multicolinearidade

$$\text{Multicolinearidade} \neq \text{colinearidade perfeita}$$

. . . 

- Multicolinearidade tem um efeito na variância dos estimadores, fazendo com que sejam grandes demais.
- Lidar com o problema não é trivial.
- Uma forma de detetar multicolnearidade é através do **fator de inflação da variância (FIV)**, $$FIV_j = \dfrac{1}{R_j^2},$$ em que $R_j^2$ é o coeficifiente de determinação da regressão de $X_j$ sobre todas as outras variáveis independentes.


## Multicolinearidade

```{r}
#| echo: true
library(MASS)
n <- 1000
vbetas <- matrix(NA, ncol = 3, nrow = 1000)
vbetas_m <- matrix(NA, ncol = 3, nrow = 1000)
x <- mvrnorm(n, mu = c(0,0), Sigma = matrix(c(1, 0.5, 0.5, 1), 2, 2))
x1 <- x[,1]
x2 <- x[,2]
z <- mvrnorm(n, mu = c(0,0), Sigma = matrix(c(1, 0.99, 0.99, 1), 2, 2))
z1 <- z[,1]
z2 <- z[,2]
for (i in 1:1000) {
  u <- rnorm(n)
  y1 <- 0.7 + 0.6*x1 + 0.5*x2 + u
  vbetas[i, ] <- summary(lm(y1 ~ x1 + x2))$coefficients[,2]
  y2 <- 0.7 + 0.6*z1 + 0.5*z2 + u
  vbetas_m[i, ] <- summary(lm(y2 ~ z1 + z2))$coefficients[,2]
}
par(mfrow = c(1, 2))
boxplot(vbetas, ylim = c(0.0, 0.3))
title("Boxplot para betas")
boxplot(vbetas_m, ylim = c(0.0, 0.3))
title("Boxplot para betas + multicolinearidade")
```


## Multicolinearidade

Para ilustrar melhor a consequência da multicolinearidade, pense no seguinte modelo de regressão $$Y = \beta_1 X_1 + \beta_2 X_2 + u.$$

. . . 

Sabemos que $\mathbb{V}(\hat{\beta}|\textbf{X}) = \sigma^2 (\textbf{X}'\textbf{X})^{-1} = \dfrac{\sigma^2}{n} (\dfrac{1}{n}\textbf{X}'\textbf{X})^{-1}$

. . . 


$$\dfrac{1}{n}\textbf{X}'\textbf{X} = \begin{pmatrix}
1 & \rho \\
\rho & 1 
\end{pmatrix}$$

. . . 

$$\mathbb{V}(\hat{\beta}|\textbf{X}) = \dfrac{\sigma^2}{n} \begin{pmatrix}
1 & \rho \\
\rho & 1 
\end{pmatrix}^{-1} = \dfrac{\sigma^2}{n (1-\rho^2)} \begin{pmatrix}
1 & -\rho \\
-\rho & 1 
\end{pmatrix}$$

. . . 

> A medida que $\rho \rightarrow \pm 1$, a variância de $\hat{\beta}$ cresce sem limites.

