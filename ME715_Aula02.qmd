---
title: "ME715 - Econometria"
subtitle: "Modelo de Regressão Linear"
author: "Prof. Carlos Trucíos </br> ctrucios@unicamp.br"
Email: "ctrucios@unicamp.br"
institute: "Instituto de Matemática, Estatística e Computação Científica (IMECC), </br> Universidade Estadual de Campinas (UNICAMP)."
knitr:
    opts_chunk: 
      fig.align: 'center'
execute:
    message: false
    warning: false
format:
    revealjs:
        slide-number: true
        show-slide-number: print
        self-contained: false
        chalkboard: true
        width: 1600
        height: 900
        theme: [default, styles.scss]
        incremental: true
        code-fold: true
        logo: "imagens/imecc.png"
        footer: "Carlos Trucíos (IMECC/UNICAMP)  |  ME715 - Econometria  |  [ctruciosm.github.io](https://ctruciosm.github.io/)"
        highlight-style: "a11y"
        title-slide-attributes:
            data-background-image: "imagens/unicamp.png"
            data-background-size: 20%
            data-background-position: 99% 5%
            data-background-opacity: "1"
---

# O modelo

## O modelo

::: columns
::: {.column width="40%"}
<center>
![](imagens/Francis_Galton.jpg)
</center>
:::

::: {.column width="60%"}

Embora Legendre e Gauss tenham utilizado o método de mínimos quadrados muito antes do que Galton (a começos de 1800), o termo "regressão" aparece por primeira vez em 1886, no trabalho de Francis Galton intitulado [_Regression   towards   mediocrity  in  hereditary  stature_](https://www.jstor.org/stable/2841583?casa_token=Hbb88KLhX8kAAAAA%3AAAJt5rJGYRZdDJNiqMigl6XspV56TVYrSjcrGHis2NRsF55BUR_8m5RUpVpf-Mg1PCtNALWYY0DVjP_mE3-uYU8QZucaVsLDp4KvzbKwdLxadXgW21p1) e é a este a quem lhe é atribuido o método.
:::
:::



::: aside
Um interessante passeio pela história de Galton e o modelo de regressão pode ser encontrado [aqui.](https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2011.00509.x)
:::




## O modelo

Quando trabalhamos com modelos de regressão,  estamos interessados em "explicar Y em termos de **X**" ou "estudar como Y varia com variações em **X**".

. . . 


::: columns
::: {.column width="45%"}
#### Regressão Linear Simples

$$Y = \beta_0 + \beta_1 X_1 + u$$


:::

::: {.column width="55%"}
#### Regressão Linear Multipla

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k + u$$

:::
:::

. . . 

| Y | Xs  |
|:-------:|:--------:|
| variável dependente  |  variável independente |
| variável explicada   |  variável explicativa |
| variável resposta    |  variável de controle |
| variável prevista    |  variável previsora   |
| regressando          | regressor |
| variável *target*    | covariável / _feature_ |

. . . 

**$u$ é o termo de erro ou perturbação, representa outros fatores (não observados), além dos $X$, que afetam $Y$.**

## O modelo

Pense no modelo de regressão linear simples. Se todos os outros fatores em $u$ permanecerem fixos, $$\Delta Y = \beta_1 \Delta X_1, \quad \text{pois} \quad \Delta u = 0.$$

. . . 

-  A variação de $Y$ é $\beta_1$ multiplicado pela variação em $X_1$.
- $\beta_1$ é o parâmetro de inclinação da relação entre $Y$ e $X_1$ (mantendo fixos todos os outros fatores).
- $\beta_0$ é chamado parâmetro de intercepto


## O modelo


[Dado que $\beta_1$ mede o efeito de $X_1$ sobre $Y$ mantendo todos os outros fatores fixos, então é facil tirar conclusões _ceteris paribus_, certo?]{style="color:blue;"}


<center>
![](imagens/gatinho_shrek.jpg)
</center>

. . . 

[**Infelizmente não.**]{style="color:red;"}

. . . 

[Como podemos aprender algo sobre o efeito _ceteris paribus_ de $X_1$ sobre $Y$ mantendo os outros fatores fixos se estamos ignorando todos os outros fatores?]{style="color:red;"}


## O modelo


<center>

**Então, como podemos estar certos de que estamos capturando uma relação _ceteris paribus_ entre Y e  $X_1$?**

![](imagens/homero_duvida.gif)

</center>

. . . 

Precisamos de algumas suposições (hipóteses do modelo) que restrinjam a maneira como $u$ está relacionado com $X_1$ (respectivamente $\textbf{X}$)

# Hipóteses do modelo
## Hipóteses do modelo

::: callout-note

- **HRLM1**: O modelo populacional é linear nos parâmetros, $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k + u$ (equivalentemete, em forma matrcial, $Y = \textbf{X} \beta + \textbf{u}$).
- **HRLM2**: $(Y_1, X_{1,1}, \ldots, X_{1,k}), \cdots, (Y_n, X_{n,1}, \ldots, X_{n,k})$ constituem uma a.a. de tamanho $n$ do modelo populacional.
- **HRLM3**: Não existe colinearidade perfeita entre as variáveis independentes e nenhuma das variáveis independentes é constante.
- **HRLM4**: $\mathbb{E}(\textbf{u}|\textbf{X}) = 0$
- **HRLM5**: Os erros tem variância constante (homocedasticidade), $\mathbb{V}(\textbf{u}|\textbf{X}) = \sigma^2 \textbf{I}$
:::


. . . 


**Estas hipóteses são conhecidas como as hipóteses de Gauss-Markov.**





# Interpretação
## Interpretação

Aplicando $\mathbb{E}(\cdot|X)$ no modelo de RLS,

$$\mathbb{E}(Y|X) = \beta_0 + \beta_1 X,$$

. . . 

ou seja, o aumento em uma unidade em $X$ faz com que o **valor esperado** de $Y$ varie em $\beta_1$ unidades. Por outro lado, o **valor esperado** de $Y$ quando $X = 0$ é $\beta_0$.

. . . 

No caso do modelo de RLM, $$\mathbb{E}(Y|\textbf{X}) = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k,\quad ou \quad \mathbb{E}(Y|\textbf{X}) = \textbf{X} \beta$$

e temos uma interpretação semelhante.


## Interpretação


Na prática, nunca conhecemos os $\beta$s e os estimamos. Assim, 


::: columns
::: {.column width="50%"}
#### Regressão Linear Simples

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i},$$

- $\hat{u}_i = y_i  - \hat{y}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i$.
- $\Delta \hat{y} = \hat{\beta}_1 \Delta x$ ($\hat{\beta}_1$ nos diz quanto varia $\hat{y}$ quando $x$ aumenta em uma unidade).
- Quando $X = x_i$, estimamos o valor de $y_i$ em $\hat{\beta}_0 + \hat{\beta}_1 x_i$.


:::

::: {.column width="50%"}
#### Regressão Linear Multipla

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i,1} + \cdots + \hat{\beta}_k x_{i,k},$$

- $\hat{u}_i = y_i  - \hat{y}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i,1} - \cdots - \hat{\beta}_k x_{i,k}$.
- $\Delta \hat{y} = \hat{\beta}_1 \Delta x_1$ ($\hat{\beta}_1$ nos diz quando varia $\hat{y}$ quando $x_1$ aumenta em uma unidade e todos os outros fatores permanecem fixos.).
- Quando $\textbf{X} = \textbf{x}_i$, estimamos o valor de $y_i$ em $\hat{\beta}_0 + \hat{\beta}_1 x_{i,1} + \cdots + \hat{\beta}_k x_{i,k}$.

:::
:::



## Interpretação

::: {.callout-warning}
::: {.nonincremental}
- $\hat{u}_i = y_i - \hat{y}_i$ é chamado de resíduo.
- Resíduo ($\hat{u}_i$) e Erro ($u_i$) **não são a mesma coisa!**
- De fato, é facil ver que $\hat{u}_i = u_i + (\beta_1 - \hat{\beta}_1) x_{i,1} + \cdots + (\beta_k - \hat{\beta}_k) x_{i,k}$
:::
:::


## Interpretação

#### Case:

Imagine que estamos interessamos em saber se os anos de educação formal (*educ*) influenciam o salário-hora (*wage*). Para isto, utilizamos o conjunto de datos _WAGE1_, que contém informação de 516 pessoas que fazem parte da população economicamente ativa dos Estados Unidos em 1976.

. . . 

```{r}
#| echo: true
library(wooldridge)
library(dplyr)
glimpse(wage1)
```

## Interpretação

Como queremos $$\underbrace{wage}_{Y} = \beta_0 + \beta_1 \underbrace{educ}_{X} + u,$$ utilizamos um modelo de regressão linear simples.

. . . 

```{r}
#| echo: true
modelo <- lm(wage ~ educ, data = wage1)
coef(modelo)
```


## Interpretação

::: columns
::: {.column width="40%"}
```{r}
#| echo: true
coef(modelo)
```
:::

::: {.column width="60%"}

::: {.incremental}
- Isso significa que uma pessoa sem nenhum ano de eduacação formal  tem um salário-hora de $\approx$ USD\$-0.90? 
- Isso faz sentido? O que poderia estar acontecendo?
:::

:::
:::

. . . 

**Resultados contra-intuitivos precisam de maior atenção**. Se avaliarmos a amostra, temos pouquissimas observações com níveis de educação baixos. Assim, não é de se surpreender que a reta de regressão não faça boas predições para níveis de educação muito baixos. 

```{r}
#| echo: true
table(wage1$educ)
```


# Estimação
## Estimação

No caso mais geral (regressão linear multipla), pode-se provar que o estimador MQO é dado por $$\hat{\beta} = (X'X)^{-1}X'Y$$

. . .

**Demostração: (no quadro) **


## Estimação

Na prática, utilizamos algum software para fazer a estimação e não nos preocupamos com a implementação (**embora, saber implementar é importante**).


::: {.panel-tabset}

## R

```{r}
#| echo: true
library(wooldridge)

modelo <- lm(wage ~ educ, data = wage1)
coef(modelo)
```


## Python

```{python}
#| echo: true
import wooldridge as woo
import statsmodels.formula.api as smf

wage1 = woo.dataWoo('wage1')
modelo = smf.ols(formula='wage ~ educ', data = wage1)
results = modelo.fit()
results.params
```


## Julia

```{julia}
#| echo: true
using WooldridgeDatasets, DataFrames, GLM

wage1 = DataFrame(wooldridge("wage1"));
modelo = lm(@formula(wage ~ educ), wage1);
coef(modelo)  
```

:::

# Propriedades
## Propriedades

Além das suas propriedades assintóticas (que veremos na próxima aula), o estimador MQO possui algumas propriedades interessantes, entre elas:


- $\mathbb{E}(\hat{\beta}) = \beta$
- $\mathbb{V}(\hat{\beta}|\textbf{X}) = \sigma^2 (X'X)^{-1}$
- $\displaystyle \sum_{i = 1}^n \hat{u}_i = 0$
- $\displaystyle \sum_{i = 1}^n x_{i,k} \hat{u}_i = 0,\quad \forall k$




# Teorema de Gauss-Markov
## Teorema de Gauss-Markov


::: {.callout-tip icon=false}
## Teorema
Sob HRLM1--HRLM5, $\hat{\beta}$ é o melhor estimador linear não viesado de $\beta$.
::: 


**Demostração (no quadro)**



::: aside
Um passo a passo da demostração pode também ser encontrado [aqui](https://ctruciosm.github.io/posts/2021-02-28-teorema-de-gauss-markov/).
:::


